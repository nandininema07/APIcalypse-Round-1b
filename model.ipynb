{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a35b09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at ./models/layoutlmv3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 1. Using OCR fallback.\n",
      "OCR failed to extract text from page 1. Skipping.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 7. Using OCR fallback.\n",
      "OCR failed to extract text from page 7. Skipping.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 0. Using OCR fallback.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Adobe Hackathon Round 1B - PDF Document Intelligence System\n",
    "Context-aware extraction and ranking of document sections using LayoutLMv3 for heading detection\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document analyzer with BERT-Tiny and LayoutLMv3 models\"\"\"\n",
    "        try:\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model = BertModel.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model.eval()\n",
    "            logger.info(\"BERT-Tiny model loaded successfully\")\n",
    "\n",
    "            self.layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"./models/layoutlmv3\", apply_ocr=False)\n",
    "            self.layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"./models/layoutlmv3\", ignore_mismatched_sizes=True)\n",
    "            \n",
    "            self.layoutlm_model.eval()\n",
    "            logger.info(\"LayoutLMv3 model and processor loaded successfully\")\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK punkt data\")\n",
    "                nltk.download('punkt', quiet=True)\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK averaged_perceptron_tagger_eng data\")\n",
    "                nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_domain_terms(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract up to 8 domain-specific terms\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        inputs = self.bert_tokenizer(combined_text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state[0]\n",
    "            tokens = self.bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "        valid_tokens = [token for i, token in enumerate(tokens) if not token.startswith('##') and token not in stop_words and len(token) >= 3 and token not in ['[CLS]', '[SEP]']]\n",
    "        valid_embeddings = [token_embeddings[i].numpy() for i, token in enumerate(tokens) if not token.startswith('##') and token not in stop_words and len(token) >= 3 and token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "        if not valid_tokens:\n",
    "            return []\n",
    "\n",
    "        embeddings_array = np.array(valid_embeddings)\n",
    "        n_clusters = min(8, len(valid_tokens))\n",
    "        if n_clusters < 2:\n",
    "            return valid_tokens[:8]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "        domain_terms = []\n",
    "        tfidf = TfidfVectorizer().fit([combined_text])\n",
    "        vocab = tfidf.vocabulary_\n",
    "        tfidf_scores = tfidf.transform([combined_text]).toarray()[0]\n",
    "\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "            if cluster_indices:\n",
    "                cluster_tokens = [valid_tokens[i] for i in cluster_indices]\n",
    "                token_scores = [(token, tfidf_scores[vocab.get(token, 0)]) for token in cluster_tokens]\n",
    "                token_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                domain_terms.append(token_scores[0][0])\n",
    "\n",
    "        tagged_terms = nltk.pos_tag(domain_terms)\n",
    "        domain_terms = [term for term, pos in tagged_terms if pos.startswith(('NN', 'VB', 'JJ')) and len(term) >= 4]\n",
    "        domain_terms = list(dict.fromkeys(domain_terms))[:8]\n",
    "        if not domain_terms:\n",
    "            domain_terms = valid_tokens[:8]\n",
    "\n",
    "        logger.info(f\"Extracted domain terms: {domain_terms}\")\n",
    "        return domain_terms\n",
    "\n",
    "    def extract_context_keywords(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract context keywords using domain terms\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', combined_text)\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "        keywords = [word for word in words if word not in stop_words and (any(term in word for term in domain_terms) or len(word) >= 5)]\n",
    "        keywords = list(dict.fromkeys(keywords))[:10]\n",
    "        if not keywords:\n",
    "            keywords = [word for word in words if word not in stop_words and len(word) >= 4][:10]\n",
    "        logger.info(f\"Extracted context keywords: {keywords}\")\n",
    "        return keywords\n",
    "\n",
    "    def is_bullet_point(self, text: str) -> bool:\n",
    "        \"\"\"Detect bullet points and list-like structures\"\"\"\n",
    "        bullet_patterns = [\n",
    "            r'^\\s*[•▪▫‣⁃\\u2022]\\s+', r'^\\s*[-*+]\\s+', r'^\\s*\\d+[\\.\\)]\\s+', r'^\\s*[a-zA-Z][\\.\\)]\\s+',\n",
    "            r'^\\s*(tip|note|warning|caution|important):?\\s+'\n",
    "        ]\n",
    "        return any(re.match(pattern, text, re.IGNORECASE) for pattern in bullet_patterns)\n",
    "\n",
    "    def is_valid_heading(self, text: str) -> bool:\n",
    "        \"\"\"Validate heading to reject fragments and bullet points\"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) < 3 or len(text) > 80 or len(text.split()) < 2 or len(text.split()) > 15:\n",
    "            return False\n",
    "        if re.match(r'.*[:,]$', text) or text.lower().startswith(('see ', 'refer ', 'check ', 'visit ')) or self.is_bullet_point(text):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_pdf(self, pdf_path: str) -> bool:\n",
    "        \"\"\"Validate PDF file integrity\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                PyPDF2.PdfReader(f, strict=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF validation failed for {pdf_path}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"Optimized text extraction with OCR fallback\"\"\"\n",
    "        sections = []\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "\n",
    "        if not self.validate_pdf(pdf_path):\n",
    "            logger.warning(f\"Skipping {pdf_path} due to validation failure\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 0):\n",
    "                    text = page.extract_text()\n",
    "                    if not text:\n",
    "                        logger.warning(f\"No text extracted from page {page_num}. Using OCR fallback.\")\n",
    "                        # Convert page to image for OCR\n",
    "                        page_image = page.to_image(resolution=150).original.convert(\"RGB\")\n",
    "                        text = pytesseract.image_to_string(page_image)\n",
    "\n",
    "                    if not text.strip():\n",
    "                        logger.warning(f\"OCR failed to extract text from page {page_num}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Split text into paragraphs\n",
    "                    paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 20]\n",
    "                    for para in paragraphs:\n",
    "                        first_line = para.split('\\n')[0].strip()\n",
    "                        if len(first_line.split()) >= 2 and len(para.split()) >= 10 and self.is_valid_heading(first_line):\n",
    "                            sections.append({\n",
    "                                'document': os.path.basename(pdf_path),\n",
    "                                'page': page_num,\n",
    "                                'section_title': first_line[:80],\n",
    "                                'text': para,\n",
    "                                'level': 2\n",
    "                            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "\n",
    "        logger.info(f\"Extracted {len(sections)} sections from {pdf_path}\")\n",
    "        return sections[:15]\n",
    "\n",
    "    def encode_text_bert(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        \"\"\"Encode texts with mean pooling\"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = [re.sub(r'\\s+', ' ', t.strip())[:500] for t in texts[i:i + batch_size]]\n",
    "            inputs = self.bert_tokenizer(batch_texts, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                embeddings.extend(batch_embeddings.numpy())\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_relevance_scores(self, sections: List[Dict], persona: str, job_to_be_done: str, context_keywords: List[str]) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"Compute relevance scores with enhanced keyword weighting\"\"\"\n",
    "        if not sections:\n",
    "            return []\n",
    "\n",
    "        logger.info(f\"Computing relevance scores for {len(sections)} sections\")\n",
    "        keywords_str = \" \".join(context_keywords)\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Focus on: {keywords_str}\"\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "        section_texts = [f\"{section['section_title']} {section['text']}\" for section in sections]\n",
    "        all_texts = [query] + section_texts\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        section_embeddings = embeddings[1:]\n",
    "\n",
    "        similarities = cosine_similarity(query_embedding, section_embeddings)[0]\n",
    "        adjusted_scores = []\n",
    "        for i, (sim, section) in enumerate(zip(similarities, sections)):\n",
    "            word_count = len(section['text'].split())\n",
    "            length_factor = 0.5 if word_count < 20 else min(1.5, 1 + 0.3 * np.log(word_count / 20))\n",
    "            title_quality = 1.5 if any(word in section['section_title'].lower() for word in ['form', 'fillable', 'onboarding', 'compliance', 'create', 'manage', 'signature']) else 1.0\n",
    "            section_content = f\"{section['section_title']} {section['text']}\".lower()\n",
    "            keyword_matches = sum(1 for keyword in context_keywords + domain_terms if keyword in section_content)\n",
    "            keyword_boost = 3.0 if keyword_matches >= 3 else 2.0 if keyword_matches == 2 else 1.5 if keyword_matches == 1 else 1.0\n",
    "            level_boost = 1.2 if section['level'] == 1 else 1.0 if section['level'] == 2 else 0.8\n",
    "            domain_boost = 1 + sum(0.15 for term in domain_terms if term in section_content)\n",
    "            adjusted_score = sim * length_factor * title_quality * keyword_boost * level_boost * domain_boost\n",
    "            adjusted_scores.append(adjusted_score)\n",
    "\n",
    "        scored_sections = list(zip(sections, adjusted_scores))\n",
    "        scored_sections.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored_sections[:max(7, len(scored_sections))]\n",
    "\n",
    "    def extract_key_sentences(self, text: str, persona: str, job_to_be_done: str, context_keywords: List[str], top_k: int = 5) -> str:\n",
    "        \"\"\"Extract key sentences ensuring concise output\"\"\"\n",
    "        sentences = [s.strip() for s in nltk.sent_tokenize(text) if len(s.split()) >= 15 and len(s) >= 50]\n",
    "        if len(sentences) < 2:\n",
    "            paragraphs = re.split(r'\\n{2,}', text)\n",
    "            combined_text = ' '.join([p.strip() for p in paragraphs if len(p.strip().split()) >= 15][:3])\n",
    "            sentences = [s.strip() for s in nltk.sent_tokenize(combined_text) if len(s.split()) >= 15 and len(s) >= 50]\n",
    "            if len(sentences) < 2:\n",
    "                return combined_text[:400].replace('\\n', ' ') if len(combined_text) > 50 else combined_text.replace('\\n', ' ')\n",
    "\n",
    "        logger.info(f\"Extracting {top_k} key sentences from {len(sentences)} sentences\")\n",
    "        keywords_str = \" \".join(context_keywords)\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Key aspects: {keywords_str}\"\n",
    "        all_texts = [query] + sentences\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        sentence_embeddings = embeddings[1:]\n",
    "        similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        # Adjust scores based on keyword matches and sentence quality\n",
    "        adjusted_similarities = []\n",
    "        for i, (sim, sentence) in enumerate(zip(similarities, sentences)):\n",
    "            keyword_boost = 1 + sum(0.25 for keyword in context_keywords if keyword in sentence.lower())\n",
    "            adjusted_similarities.append(sim * keyword_boost)\n",
    "\n",
    "        sentence_scores = list(enumerate(adjusted_similarities))\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_indices = [idx for idx, _ in sentence_scores[:top_k]]\n",
    "        top_indices.sort()\n",
    "\n",
    "        # Limit output to 6 lines max and remove redundant line breaks\n",
    "        selected_sentences = [sentences[i] for i in top_indices][:5]\n",
    "        return ' '.join(selected_sentences).replace('\\n', ' ')    \n",
    "\n",
    "def load_input_config(input_dir: str) -> Tuple[str, str]:\n",
    "    \"\"\"Load input configuration\"\"\"\n",
    "    config_path = os.path.join(input_dir, \"input.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"input.json not found at {config_path}\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    persona = config.get('persona')\n",
    "    job_to_be_done = config.get('job_to_be_done')\n",
    "    if isinstance(persona, dict):\n",
    "        persona = persona.get('role') or persona.get('name')\n",
    "    if isinstance(job_to_be_done, dict):\n",
    "        job_to_be_done = job_to_be_done.get('task') or job_to_be_done.get('description')\n",
    "    if not persona or not job_to_be_done:\n",
    "        raise ValueError(\"Both 'persona' and 'job_to_be_done' must be specified in input.json\")\n",
    "    return str(persona), str(job_to_be_done)\n",
    "\n",
    "def find_pdf_files(pdfs_dir: str) -> List[str]:\n",
    "    \"\"\"Find and validate PDF files\"\"\"\n",
    "    if not os.path.exists(pdfs_dir):\n",
    "        raise FileNotFoundError(f\"PDFs directory not found: {pdfs_dir}\")\n",
    "    pdf_files = [f for f in os.listdir(pdfs_dir) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in {pdfs_dir}\")\n",
    "    return [os.path.join(pdfs_dir, f) for f in pdf_files]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process documents and generate output\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        input_dir = \"./input\"\n",
    "        pdfs_dir = os.path.join(input_dir, \"PDFs\")\n",
    "        output_dir = \"./output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Loading input configuration\")\n",
    "        persona, job_to_be_done = load_input_config(input_dir)\n",
    "        logger.info(f\"Persona: {persona}\")\n",
    "        logger.info(f\"Job to be done: {job_to_be_done}\")\n",
    "\n",
    "        pdf_paths = find_pdf_files(pdfs_dir)\n",
    "        pdf_files = [os.path.basename(path) for path in pdf_paths]\n",
    "        logger.info(f\"Processing {len(pdf_files)} documents: {pdf_files}\")\n",
    "\n",
    "        logger.info(\"Initializing document analyzer\")\n",
    "        analyzer = DocumentAnalyzer()\n",
    "\n",
    "        context_keywords = analyzer.extract_context_keywords(persona, job_to_be_done)\n",
    "        all_sections = []\n",
    "\n",
    "        for pdf_path in pdf_paths:\n",
    "            try:\n",
    "                # Pass only the required argument\n",
    "                sections = analyzer.extract_text_from_pdf(pdf_path)\n",
    "                if sections:\n",
    "                    all_sections.extend(sections)\n",
    "                else:\n",
    "                    logger.warning(f\"No sections extracted from {pdf_path}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {pdf_path}: {str(e)}. Skipping.\")\n",
    "\n",
    "        if not all_sections:\n",
    "            logger.warning(\"No sections extracted from any document. Skipping output generation.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Total sections extracted: {len(all_sections)}\")\n",
    "        scored_sections = analyzer.compute_relevance_scores(all_sections, persona, job_to_be_done, context_keywords)\n",
    "        top_sections = scored_sections\n",
    "\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"input_documents\": pdf_files,\n",
    "                \"persona\": persona,\n",
    "                \"job_to_be_done\": job_to_be_done,\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"extracted_sections\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"section_title\": section['section_title'],\n",
    "                    \"importance_rank\": rank,\n",
    "                    \"page_number\": section['page']\n",
    "                } for rank, (section, _) in enumerate(top_sections[:min(20, len(top_sections))], 1)\n",
    "            ],\n",
    "            \"sub_section_analysis\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"refined_text\": analyzer.extract_key_sentences(\n",
    "                        section['text'], persona, job_to_be_done, context_keywords\n",
    "                    ),\n",
    "                    \"page_number\": section['page']\n",
    "                } for section, _ in top_sections[:min(20, len(top_sections))]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"output.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "        logger.info(f\"Output saved to {output_path}\")\n",
    "        for rank, (section, score) in enumerate(top_sections, 1):\n",
    "            logger.info(f\"{rank}. {section['document']} - {section['section_title']} (Level: {section['level']}, Score: {score:.3f})\")\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3680c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
