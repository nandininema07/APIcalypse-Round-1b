{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a35b09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at ./models/layoutlmv3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 1. Using OCR fallback.\n",
      "OCR failed to extract text from page 1. Skipping.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 7. Using OCR fallback.\n",
      "OCR failed to extract text from page 7. Skipping.\n",
      "No text extracted from page 0. Using OCR fallback.\n",
      "No text extracted from page 0. Using OCR fallback.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Adobe Hackathon Round 1B - PDF Document Intelligence System\n",
    "Context-aware extraction and ranking of document sections using LayoutLMv3 for heading detection\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document analyzer with BERT-Tiny and LayoutLMv3 models\"\"\"\n",
    "        try:\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model = BertModel.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model.eval()\n",
    "            logger.info(\"BERT-Tiny model loaded successfully\")\n",
    "\n",
    "            self.layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"./models/layoutlmv3\", apply_ocr=False)\n",
    "            self.layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"./models/layoutlmv3\", ignore_mismatched_sizes=True)\n",
    "            \n",
    "            self.layoutlm_model.eval()\n",
    "            logger.info(\"LayoutLMv3 model and processor loaded successfully\")\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK punkt data\")\n",
    "                nltk.download('punkt', quiet=True)\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK averaged_perceptron_tagger_eng data\")\n",
    "                nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_domain_terms(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract domain-specific terms using semantic similarity\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', combined_text)\n",
    "        valid_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Use embeddings to compute semantic relevance\n",
    "        inputs = self.bert_tokenizer(valid_words, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            word_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "        query_inputs = self.bert_tokenizer(combined_text, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            query_outputs = self.bert_model(**query_inputs)\n",
    "            query_embedding = query_outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "        similarities = cosine_similarity(query_embedding, word_embeddings)\n",
    "        sorted_indices = np.argsort(similarities[0])[::-1]\n",
    "        domain_terms = [valid_words[i] for i in sorted_indices[:8]]  # Top 8 terms\n",
    "        logger.info(f\"Extracted domain terms: {domain_terms}\")\n",
    "        return domain_terms\n",
    "\n",
    "    def extract_context_keywords(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract context keywords dynamically based on semantic similarity\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "\n",
    "        # Extract words from the query text\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', combined_text)\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        valid_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Filter keywords based on their presence in domain terms\n",
    "        keywords = [word for word in valid_words if any(term in word for term in domain_terms)]\n",
    "        keywords = list(dict.fromkeys(keywords))[:10]  # Limit to top 10 keywords\n",
    "        logger.info(f\"Extracted context keywords: {keywords}\")\n",
    "        return keywords\n",
    "    \n",
    "    def classify_headings_with_layoutlm(self, page_image: Image.Image) -> List[str]:\n",
    "        \"\"\"Classify headings using LayoutLMv3\"\"\"\n",
    "        try:\n",
    "            encoding = self.layoutlm_processor(page_image, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.layoutlm_model(**encoding)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()\n",
    "            tokens = self.layoutlm_processor.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "            headings = [token for token, prediction in zip(tokens, predictions) if prediction == 1]  # Label 1 for headings\n",
    "            return headings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error classifying headings with LayoutLMv3: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def is_bullet_point(self, text: str) -> bool:\n",
    "        \"\"\"Detect bullet points and list-like structures\"\"\"\n",
    "        bullet_patterns = [\n",
    "            r'^\\s*[•▪▫‣⁃\\u2022]\\s+', r'^\\s*[-*+]\\s+', r'^\\s*\\d+[\\.\\)]\\s+', r'^\\s*[a-zA-Z][\\.\\)]\\s+',\n",
    "            r'^\\s*(tip|note|warning|caution|important):?\\s+'\n",
    "        ]\n",
    "        return any(re.match(pattern, text, re.IGNORECASE) for pattern in bullet_patterns)\n",
    "\n",
    "    def is_valid_heading(self, text: str) -> bool:\n",
    "        \"\"\"Validate heading to reject fragments and bullet points\"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) < 3 or len(text) > 80 or len(text.split()) < 2 or len(text.split()) > 15:\n",
    "            return False\n",
    "        if re.match(r'.*[:,]$', text) or text.lower().startswith(('see ', 'refer ', 'check ', 'visit ')) or self.is_bullet_point(text):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_pdf(self, pdf_path: str) -> bool:\n",
    "        \"\"\"Validate PDF file integrity\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                PyPDF2.PdfReader(f, strict=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF validation failed for {pdf_path}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"Optimized text extraction with OCR fallback\"\"\"\n",
    "        sections = []\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "\n",
    "        if not self.validate_pdf(pdf_path):\n",
    "            logger.warning(f\"Skipping {pdf_path} due to validation failure\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 0):\n",
    "                    text = page.extract_text()\n",
    "                    if not text:\n",
    "                        # OCR fallback\n",
    "                        logger.warning(f\"No text extracted from page {page_num}. Using OCR fallback.\")\n",
    "                        page_image = page.to_image(resolution=150).original.convert(\"RGB\")\n",
    "                        text = pytesseract.image_to_string(page_image)\n",
    "\n",
    "                    if not text.strip():\n",
    "                        logger.warning(f\"OCR failed to extract text from page {page_num}. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    # Split text into paragraphs\n",
    "                    paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 20]\n",
    "                    for para in paragraphs:\n",
    "                        first_line = para.split('\\n')[0].strip()\n",
    "                        if len(first_line.split()) >= 2 and len(para.split()) >= 10 and self.is_valid_heading(first_line):\n",
    "                            sections.append({\n",
    "                                'document': os.path.basename(pdf_path),\n",
    "                                'page': page_num,\n",
    "                                'section_title': first_line[:80],\n",
    "                                'text': para,\n",
    "                                'level': 2\n",
    "                            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "\n",
    "        logger.info(f\"Extracted {len(sections)} sections from {pdf_path}\")\n",
    "        return sections[:15]\n",
    "\n",
    "    def encode_text_bert(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        \"\"\"Encode texts with mean pooling\"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = [re.sub(r'\\s+', ' ', t.strip())[:500] for t in texts[i:i + batch_size]]\n",
    "            inputs = self.bert_tokenizer(batch_texts, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                embeddings.extend(batch_embeddings.numpy())\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_relevance_scores(self, sections: List[Dict], persona: str, job_to_be_done: str, context_keywords: List[str]) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"Compute relevance scores using semantic similarity and contextual alignment\"\"\"\n",
    "        if not sections:\n",
    "            return []\n",
    "\n",
    "        logger.info(f\"Computing relevance scores for {len(sections)} sections\")\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Focus on: {', '.join(context_keywords)}\"\n",
    "        query_inputs = self.bert_tokenizer(query, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            query_outputs = self.bert_model(**query_inputs)\n",
    "            query_embedding = query_outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "        adjusted_scores = []\n",
    "        for section in sections:\n",
    "            section_text = f\"{section['section_title']} {section['text']}\"\n",
    "            section_inputs = self.bert_tokenizer(section_text, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                section_outputs = self.bert_model(**section_inputs)\n",
    "                section_embedding = section_outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "            # Compute semantic similarity\n",
    "            similarity = cosine_similarity(query_embedding, section_embedding)[0][0]\n",
    "\n",
    "            # Adjust scores based on keyword matches and section quality\n",
    "            keyword_matches = sum(1 for keyword in context_keywords if keyword.lower() in section_text.lower())\n",
    "            keyword_boost = 1 + (0.2 * keyword_matches)  # Boost score based on keyword matches\n",
    "            adjusted_score = similarity * keyword_boost\n",
    "            adjusted_scores.append((section, adjusted_score))\n",
    "\n",
    "        # Sort sections by relevance\n",
    "        scored_sections = sorted(adjusted_scores, key=lambda x: x[1], reverse=True)\n",
    "        return scored_sections[:min(20, len(scored_sections))]\n",
    "\n",
    "    def filter_sections(self, sections: List[Dict], context_keywords: List[str]) -> List[Dict]:\n",
    "        \"\"\"Filter sections based on query context\"\"\"\n",
    "        filtered_sections = []\n",
    "        for section in sections:\n",
    "            section_text = f\"{section['section_title']} {section['text']}\".lower()\n",
    "            if any(keyword.lower() in section_text for keyword in context_keywords):\n",
    "                filtered_sections.append(section)\n",
    "        return filtered_sections\n",
    "\n",
    "    def extract_key_sentences(self, text: str, persona: str, job_to_be_done: str, context_keywords: List[str], top_k: int = 5) -> str:\n",
    "        \"\"\"Extract key sentences ensuring contextual alignment\"\"\"\n",
    "        sentences = [s.strip() for s in nltk.sent_tokenize(text) if len(s.split()) >= 15 and len(s) >= 50]\n",
    "        if len(sentences) < 2:\n",
    "            return text[:400].replace('\\n', ' ')  # Fallback for short text\n",
    "\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Key aspects: {', '.join(context_keywords)}\"\n",
    "        all_texts = [query] + sentences\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        sentence_embeddings = embeddings[1:]\n",
    "        similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        # Rank sentences by relevance\n",
    "        ranked_sentences = sorted(zip(sentences, similarities), key=lambda x: x[1], reverse=True)\n",
    "        selected_sentences = [sentence for sentence, _ in ranked_sentences[:top_k]]\n",
    "        return ' '.join(selected_sentences).replace('\\n', ' ')\n",
    "\n",
    "def load_input_config(input_dir: str) -> Tuple[str, str]:\n",
    "    \"\"\"Load input configuration\"\"\"\n",
    "    config_path = os.path.join(input_dir, \"input.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"input.json not found at {config_path}\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    persona = config.get('persona')\n",
    "    job_to_be_done = config.get('job_to_be_done')\n",
    "    if isinstance(persona, dict):\n",
    "        persona = persona.get('role') or persona.get('name')\n",
    "    if isinstance(job_to_be_done, dict):\n",
    "        job_to_be_done = job_to_be_done.get('task') or job_to_be_done.get('description')\n",
    "    if not persona or not job_to_be_done:\n",
    "        raise ValueError(\"Both 'persona' and 'job_to_be_done' must be specified in input.json\")\n",
    "    return str(persona), str(job_to_be_done)\n",
    "\n",
    "def find_pdf_files(pdfs_dir: str) -> List[str]:\n",
    "    \"\"\"Find and validate PDF files\"\"\"\n",
    "    if not os.path.exists(pdfs_dir):\n",
    "        raise FileNotFoundError(f\"PDFs directory not found: {pdfs_dir}\")\n",
    "    pdf_files = [f for f in os.listdir(pdfs_dir) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in {pdfs_dir}\")\n",
    "    return [os.path.join(pdfs_dir, f) for f in pdf_files]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process documents and generate output\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        input_dir = \"./input\"\n",
    "        pdfs_dir = os.path.join(input_dir, \"PDFs\")\n",
    "        output_dir = \"./output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Loading input configuration\")\n",
    "        persona, job_to_be_done = load_input_config(input_dir)\n",
    "        logger.info(f\"Persona: {persona}\")\n",
    "        logger.info(f\"Job to be done: {job_to_be_done}\")\n",
    "\n",
    "        pdf_paths = find_pdf_files(pdfs_dir)\n",
    "        pdf_files = [os.path.basename(path) for path in pdf_paths]\n",
    "        logger.info(f\"Processing {len(pdf_files)} documents: {pdf_files}\")\n",
    "\n",
    "        logger.info(\"Initializing document analyzer\")\n",
    "        analyzer = DocumentAnalyzer()\n",
    "\n",
    "        context_keywords = analyzer.extract_context_keywords(persona, job_to_be_done)\n",
    "        all_sections = []\n",
    "\n",
    "        for pdf_path in pdf_paths:\n",
    "            try:\n",
    "                # Pass only the required argument\n",
    "                sections = analyzer.extract_text_from_pdf(pdf_path)\n",
    "                if sections:\n",
    "                    all_sections.extend(sections)\n",
    "                else:\n",
    "                    logger.warning(\"\")\n",
    "            except Exception as e:\n",
    "                logger.error(\"\")\n",
    "\n",
    "        if not all_sections:\n",
    "            logger.warning(\"No sections extracted from any document. Skipping output generation.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Total sections extracted: {len(all_sections)}\")\n",
    "        scored_sections = analyzer.compute_relevance_scores(all_sections, persona, job_to_be_done, context_keywords)\n",
    "        top_sections = scored_sections\n",
    "\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"input_documents\": pdf_files,\n",
    "                \"persona\": persona,\n",
    "                \"job_to_be_done\": job_to_be_done,\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"extracted_sections\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"section_title\": section['section_title'],\n",
    "                    \"importance_rank\": rank,\n",
    "                    \"page_number\": section['page']\n",
    "                } for rank, (section, _) in enumerate(top_sections[:min(20, len(top_sections))], 1)\n",
    "            ],\n",
    "            \"sub_section_analysis\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"refined_text\": analyzer.extract_key_sentences(\n",
    "                        section['text'], persona, job_to_be_done, context_keywords\n",
    "                    ),\n",
    "                    \"page_number\": section['page']\n",
    "                } for section, _ in top_sections[:min(20, len(top_sections))]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"output.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "        logger.info(f\"Output saved to {output_path}\")\n",
    "        for rank, (section, score) in enumerate(top_sections, 1):\n",
    "            logger.info(f\"{rank}. {section['document']} - {section['section_title']} (Level: {section['level']}, Score: {score:.3f})\")\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#THIS IS GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66daa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
