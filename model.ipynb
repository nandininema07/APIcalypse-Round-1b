{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a35b09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 02:53:03,391 - INFO - Loading input configuration\n",
      "2025-07-27 02:53:03,394 - INFO - Persona: HR professional\n",
      "2025-07-27 02:53:03,394 - INFO - Job to be done: Create and manage fillable forms for onboarding and compliance.\n",
      "2025-07-27 02:53:03,396 - INFO - Processing 15 documents: ['Learn Acrobat - Create and Convert_1.pdf', 'Learn Acrobat - Create and Convert_2.pdf', 'Learn Acrobat - Edit_1.pdf', 'Learn Acrobat - Edit_2.pdf', 'Learn Acrobat - Export_1.pdf', 'Learn Acrobat - Export_2.pdf', 'Learn Acrobat - Fill and Sign.pdf', 'Learn Acrobat - Generative AI_1.pdf', 'Learn Acrobat - Generative AI_2.pdf', 'Learn Acrobat - Request e-signatures_1.pdf', 'Learn Acrobat - Request e-signatures_2.pdf', 'Learn Acrobat - Share_1.pdf', 'Learn Acrobat - Share_2.pdf', 'Test Your Acrobat Exporting Skills.pdf', 'The Ultimate PDF Sharing Checklist.pdf']\n",
      "2025-07-27 02:53:03,397 - INFO - Initializing document analyzer\n",
      "2025-07-27 02:53:03,456 - INFO - BERT-Tiny model loaded successfully\n",
      "2025-07-27 02:53:03,472 - INFO - Round 1A heading detection model loaded successfully\n",
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at ./models/layoutlmv3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-07-27 02:53:03,746 - INFO - LayoutLMv3 model and processor loaded successfully\n",
      "2025-07-27 02:53:03,781 - INFO - Extracted domain terms: ['manage', 'forms', 'professional', 'onboard', 'compliance', 'create', 'fill']\n",
      "2025-07-27 02:53:03,783 - INFO - Extracted context keywords: ['professional', 'create', 'manage', 'fillable', 'forms', 'onboarding', 'compliance']\n",
      "2025-07-27 02:53:03,783 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Create and Convert_1.pdf\n",
      "2025-07-27 02:53:05,402 - WARNING - No text extracted from page 1\n",
      "c:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1731: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "2025-07-27 02:53:23,053 - INFO - Extracted 15 sections from ./input\\PDFs\\Learn Acrobat - Create and Convert_1.pdf\n",
      "2025-07-27 02:53:23,056 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Create and Convert_2.pdf\n",
      "2025-07-27 02:53:36,036 - INFO - Extracted 9 sections from ./input\\PDFs\\Learn Acrobat - Create and Convert_2.pdf\n",
      "2025-07-27 02:53:36,036 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Edit_1.pdf\n",
      "2025-07-27 02:53:36,876 - WARNING - No text extracted from page 1\n",
      "2025-07-27 02:53:46,076 - INFO - Extracted 2 sections from ./input\\PDFs\\Learn Acrobat - Edit_1.pdf\n",
      "2025-07-27 02:53:46,076 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Edit_2.pdf\n",
      "2025-07-27 02:53:47,277 - WARNING - No text extracted from page 2\n",
      "2025-07-27 02:53:56,763 - INFO - Extracted 5 sections from ./input\\PDFs\\Learn Acrobat - Edit_2.pdf\n",
      "2025-07-27 02:53:56,766 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Export_1.pdf\n",
      "2025-07-27 02:53:57,643 - WARNING - No text extracted from page 1\n",
      "2025-07-27 02:54:16,041 - INFO - Extracted 14 sections from ./input\\PDFs\\Learn Acrobat - Export_1.pdf\n",
      "2025-07-27 02:54:16,045 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Export_2.pdf\n",
      "2025-07-27 02:54:18,496 - INFO - Extracted 1 sections from ./input\\PDFs\\Learn Acrobat - Export_2.pdf\n",
      "2025-07-27 02:54:18,496 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Fill and Sign.pdf\n",
      "2025-07-27 02:54:30,207 - INFO - Extracted 7 sections from ./input\\PDFs\\Learn Acrobat - Fill and Sign.pdf\n",
      "2025-07-27 02:54:30,207 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Generative AI_1.pdf\n",
      "2025-07-27 02:54:31,064 - WARNING - No text extracted from page 1\n",
      "2025-07-27 02:54:36,372 - WARNING - No text extracted from page 8\n",
      "2025-07-27 02:54:48,701 - INFO - Extracted 9 sections from ./input\\PDFs\\Learn Acrobat - Generative AI_1.pdf\n",
      "2025-07-27 02:54:48,703 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Generative AI_2.pdf\n",
      "2025-07-27 02:54:59,191 - INFO - Extracted 2 sections from ./input\\PDFs\\Learn Acrobat - Generative AI_2.pdf\n",
      "2025-07-27 02:54:59,196 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Request e-signatures_1.pdf\n",
      "2025-07-27 02:55:00,126 - WARNING - No text extracted from page 1\n",
      "2025-07-27 02:55:18,279 - INFO - Extracted 13 sections from ./input\\PDFs\\Learn Acrobat - Request e-signatures_1.pdf\n",
      "2025-07-27 02:55:18,281 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Request e-signatures_2.pdf\n",
      "2025-07-27 02:55:26,570 - INFO - Extracted 7 sections from ./input\\PDFs\\Learn Acrobat - Request e-signatures_2.pdf\n",
      "2025-07-27 02:55:26,572 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Share_1.pdf\n",
      "2025-07-27 02:55:27,429 - WARNING - No text extracted from page 1\n",
      "2025-07-27 02:55:35,269 - INFO - Extracted 2 sections from ./input\\PDFs\\Learn Acrobat - Share_1.pdf\n",
      "2025-07-27 02:55:35,269 - INFO - Processing PDF: ./input\\PDFs\\Learn Acrobat - Share_2.pdf\n",
      "2025-07-27 02:55:45,800 - INFO - Extracted 7 sections from ./input\\PDFs\\Learn Acrobat - Share_2.pdf\n",
      "2025-07-27 02:55:45,803 - INFO - Processing PDF: ./input\\PDFs\\Test Your Acrobat Exporting Skills.pdf\n",
      "2025-07-27 02:55:47,223 - INFO - Extracted 0 sections from ./input\\PDFs\\Test Your Acrobat Exporting Skills.pdf\n",
      "2025-07-27 02:55:47,226 - INFO - Processing PDF: ./input\\PDFs\\The Ultimate PDF Sharing Checklist.pdf\n",
      "2025-07-27 02:55:49,186 - INFO - Extracted 2 sections from ./input\\PDFs\\The Ultimate PDF Sharing Checklist.pdf\n",
      "2025-07-27 02:55:49,186 - INFO - Total sections extracted: 95\n",
      "2025-07-27 02:55:49,186 - INFO - Computing relevance scores for 95 sections\n",
      "2025-07-27 02:55:49,218 - INFO - Extracted domain terms: ['manage', 'forms', 'professional', 'onboard', 'compliance', 'create', 'fill']\n",
      "2025-07-27 02:55:49,350 - INFO - Processing completed in 165.96 seconds\n",
      "2025-07-27 02:55:49,350 - INFO - Output saved to ./output\\output.json\n",
      "2025-07-27 02:55:49,350 - INFO - 1. Learn Acrobat - Fill and Sign.pdf - Fill In Interactive Forms (Level: 1, Score: 6.007)\n",
      "2025-07-27 02:55:49,350 - INFO - 2. Learn Acrobat - Create and Convert_2.pdf - Create A Pdf Using The Print Command (Windows) (Level: 1, Score: 1.743)\n",
      "2025-07-27 02:55:49,350 - INFO - 3. Learn Acrobat - Create and Convert_1.pdf - Create A Pdf As An Email Attachment (Level: 1, Score: 1.675)\n",
      "2025-07-27 02:55:49,350 - INFO - 4. Learn Acrobat - Create and Convert_1.pdf - What’S The Best Way To Create A Pdf? (Level: 1, Score: 1.631)\n",
      "2025-07-27 02:55:49,350 - INFO - 5. Learn Acrobat - Fill and Sign.pdf - You Can Change A Flat Form To A Fillable Form By Either Using The Prepare Form Tool Or (Level: 2, Score: 1.558)\n",
      "2025-07-27 02:55:49,350 - INFO - 6. Learn Acrobat - Fill and Sign.pdf - The Forms Preferences Apply To The Way That The Application Handles Open Forms As You Work. (Level: 2, Score: 1.524)\n",
      "2025-07-27 02:55:49,356 - INFO - 7. Learn Acrobat - Create and Convert_2.pdf - Creates An Embedded Index, Which Speeds Up Searches, Especially When You Convert Large (Level: 2, Score: 1.431)\n",
      "2025-07-27 02:55:49,356 - INFO - 8. Learn Acrobat - Create and Convert_1.pdf - Excel Files To Pdf (Level: 1, Score: 1.188)\n",
      "2025-07-27 02:55:49,356 - INFO - 9. Learn Acrobat - Request e-signatures_1.pdf - Certifying And Signing Documents (Level: 1, Score: 1.163)\n",
      "2025-07-27 02:55:49,356 - INFO - 10. Learn Acrobat - Fill and Sign.pdf - Automatically. Type Your Text To Fill The Field. (Level: 2, Score: 1.117)\n",
      "2025-07-27 02:55:49,356 - INFO - 11. Learn Acrobat - Export_1.pdf - Includes Full Dsc (Document Structuring Conventions) Comments And Other Advanced (Level: 2, Score: 0.974)\n",
      "2025-07-27 02:55:49,356 - INFO - 12. Learn Acrobat - Export_1.pdf - Convert Pdf To Xml 1.0 (Level: 1, Score: 0.965)\n",
      "2025-07-27 02:55:49,360 - INFO - 13. Learn Acrobat - Create and Convert_1.pdf - Or, Select The Hamburger Menu (Windows) Or (Level: 2, Score: 0.949)\n",
      "2025-07-27 02:55:49,360 - INFO - 14. Learn Acrobat - Export_1.pdf - Format, Or Export Images In A Pdf To Another Format. (Level: 2, Score: 0.937)\n",
      "2025-07-27 02:55:49,360 - INFO - 15. Learn Acrobat - Create and Convert_1.pdf - Or, Select The Hamburger Menu (Windows) Or (Level: 2, Score: 0.905)\n",
      "2025-07-27 02:55:49,360 - INFO - 16. Learn Acrobat - Edit_2.pdf - Specify The Maximum File Size For Each Document In The Split. (Level: 2, Score: 0.885)\n",
      "2025-07-27 02:55:49,360 - INFO - 17. Learn Acrobat - Create and Convert_1.pdf - Depending On The Type Of File Being Converted, The Authoring Applica�On Opens (Level: 2, Score: 0.873)\n",
      "2025-07-27 02:55:49,360 - INFO - 18. Learn Acrobat - Create and Convert_2.pdf - Converts Certain Elements In Original Office Documents To Pdf Bookmarks: Word Headings, (Level: 2, Score: 0.851)\n",
      "2025-07-27 02:55:49,366 - INFO - 19. Learn Acrobat - Export_1.pdf - Rgb/Grayscale (Level: 2, Score: 0.677)\n",
      "2025-07-27 02:55:49,366 - INFO - 20. Learn Acrobat - Export_1.pdf - Rgb/Cmyk/Grayscale (Level: 2, Score: 0.671)\n",
      "2025-07-27 02:55:49,366 - INFO - 21. Learn Acrobat - Export_1.pdf - Convert Pdf To Jpeg Or Jpeg 2000 (Level: 1, Score: 0.501)\n",
      "2025-07-27 02:55:49,369 - INFO - 22. Learn Acrobat - Export_1.pdf - File Format Op�Ons For Pdf Export In Acrobat (Level: 1, Score: 0.494)\n",
      "2025-07-27 02:55:49,370 - INFO - 23. Learn Acrobat - Request e-signatures_2.pdf - Set The Trust Level Of A Certificate (Level: 1, Score: 0.487)\n",
      "2025-07-27 02:55:49,371 - INFO - 24. Learn Acrobat - Create and Convert_1.pdf - Convert Email Folders To A New Pdf (Level: 1, Score: 0.486)\n",
      "2025-07-27 02:55:49,373 - INFO - 25. Learn Acrobat - Generative AI_2.pdf - Clear Chat History (Level: 1, Score: 0.482)\n",
      "2025-07-27 02:55:49,373 - INFO - 26. Learn Acrobat - Create and Convert_2.pdf - Adobe Pdf Printing Preferences (Windows) (Level: 1, Score: 0.471)\n",
      "2025-07-27 02:55:49,373 - INFO - 27. Learn Acrobat - Export_2.pdf - Take A Snapshot Of A Page (Level: 1, Score: 0.466)\n",
      "2025-07-27 02:55:49,373 - INFO - 28. The Ultimate PDF Sharing Checklist.pdf - Collaborating On A Project, Seeking Feedback, Or Simply Distributing (Level: 2, Score: 0.454)\n",
      "2025-07-27 02:55:49,376 - INFO - 29. Learn Acrobat - Generative AI_2.pdf - Read-Aloud Generated Responses (Level: 1, Score: 0.450)\n",
      "2025-07-27 02:55:49,376 - INFO - 30. Learn Acrobat - Request e-signatures_1.pdf - Store Certificates On Directory Servers (Level: 1, Score: 0.445)\n",
      "2025-07-27 02:55:49,376 - INFO - 31. Learn Acrobat - Request e-signatures_1.pdf - Particular Order For Signing, Toggle The Switch To Complete In Any Order. (Level: 2, Score: 0.445)\n",
      "2025-07-27 02:55:49,376 - INFO - 32. Learn Acrobat - Request e-signatures_1.pdf - Alternatively, From Acrobat Home, Select See All Tools. In The Protect Section, (Level: 2, Score: 0.444)\n",
      "2025-07-27 02:55:49,376 - INFO - 33. Learn Acrobat - Request e-signatures_1.pdf - For Instance, Companies In The European Union Who Need To Comply With Advanced Or (Level: 2, Score: 0.443)\n",
      "2025-07-27 02:55:49,376 - INFO - 34. Learn Acrobat - Request e-signatures_1.pdf - Determine The Type You Should Choose To Sign Your Document. (See Signature Types.) (Level: 2, Score: 0.442)\n",
      "2025-07-27 02:55:49,376 - INFO - 35. Learn Acrobat - Request e-signatures_1.pdf - Select Next, And Then Select Finish. (Level: 2, Score: 0.442)\n",
      "2025-07-27 02:55:49,376 - INFO - 36. Learn Acrobat - Share_2.pdf - Commenting Preferences (Level: 1, Score: 0.435)\n",
      "2025-07-27 02:55:49,381 - INFO - 37. Learn Acrobat - Export_1.pdf - The Pdf Optimizer Lets You Change The Compatibility Version Of Your Pdfs So They Can Be (Level: 2, Score: 0.434)\n",
      "2025-07-27 02:55:49,381 - INFO - 38. Learn Acrobat - Export_1.pdf - Menu Next To Microsoft Excel, Select The Format You Want Your Excel To Be. You (Level: 2, Score: 0.434)\n",
      "2025-07-27 02:55:49,383 - INFO - 39. Learn Acrobat - Request e-signatures_2.pdf - Extended Pdf. (Level: 2, Score: 0.434)\n",
      "2025-07-27 02:55:49,383 - INFO - 40. Learn Acrobat - Request e-signatures_2.pdf - You Can Sign Component Pdfs Within A Pdf Portfolio, Or Sign The Pdf Portfolio As A Whole. (Level: 2, Score: 0.434)\n",
      "2025-07-27 02:55:49,384 - INFO - 41. Learn Acrobat - Request e-signatures_2.pdf - The Component Pdfs, You Can Sign The Entire Pdf Portfolio To Finalize It. Alternatively, You Can (Level: 2, Score: 0.433)\n",
      "2025-07-27 02:55:49,384 - INFO - 42. Learn Acrobat - Request e-signatures_2.pdf - From Ldap Directories. (Level: 2, Score: 0.433)\n",
      "2025-07-27 02:55:49,385 - INFO - 43. Learn Acrobat - Export_1.pdf - Pdf To Tiff Conversion Settings (Level: 2, Score: 0.433)\n",
      "2025-07-27 02:55:49,385 - INFO - 44. Learn Acrobat - Request e-signatures_1.pdf - When You Apply A Certificate-Based Signature, Acrobat Uses A Hashing Algorithm To Generate A (Level: 2, Score: 0.433)\n",
      "2025-07-27 02:55:49,386 - INFO - 45. The Ultimate PDF Sharing Checklist.pdf - The Ultimate Pdf Sharing Checklist (Level: 2, Score: 0.429)\n",
      "2025-07-27 02:55:49,386 - INFO - 46. Learn Acrobat - Create and Convert_1.pdf - Appropriate File Type And Follow Through The Steps. (Level: 2, Score: 0.428)\n",
      "2025-07-27 02:55:49,387 - INFO - 47. Learn Acrobat - Request e-signatures_1.pdf - Specify The Reason For Signing The Document, The Location, And Your Contact (Level: 2, Score: 0.425)\n",
      "2025-07-27 02:55:49,388 - INFO - 48. Learn Acrobat - Generative AI_1.pdf - With Predefined Categories Using Named Entity Recognition, E.G., Replacing “John (Level: 2, Score: 0.425)\n",
      "2025-07-27 02:55:49,388 - INFO - 49. Learn Acrobat - Request e-signatures_1.pdf - Certify Options Provide A Higher Level Of Document Control Than Digitally Sign. For Documents (Level: 2, Score: 0.424)\n",
      "2025-07-27 02:55:49,390 - INFO - 50. Learn Acrobat - Share_2.pdf - And Markups In Pdfs. (Level: 2, Score: 0.423)\n",
      "2025-07-27 02:55:49,390 - INFO - 51. Learn Acrobat - Generative AI_1.pdf - Similarities, Differences, And Key Patterns. With Ai Assistant, You Can Make Targeted Requests, (Level: 2, Score: 0.422)\n",
      "2025-07-27 02:55:49,390 - INFO - 52. Learn Acrobat - Generative AI_1.pdf - Primary Question. Ai Assistant Uses The Latest Ai Language Model, Gpt 4O, To Provide More (Level: 2, Score: 0.421)\n",
      "2025-07-27 02:55:49,391 - INFO - 53. Learn Acrobat - Create and Convert_1.pdf - Corresponding Data File To Output Mail Merges Directly To Pdf. You Can Even Set Up Pdfmaker To (Level: 2, Score: 0.421)\n",
      "2025-07-27 02:55:49,392 - INFO - 54. Learn Acrobat - Request e-signatures_2.pdf - Limited Time Because Certificates Related To The Signature Eventually Expire Or Are Revoked. (Level: 2, Score: 0.419)\n",
      "2025-07-27 02:55:49,392 - INFO - 55. Learn Acrobat - Generative AI_1.pdf - Using Ai Assistant In Acrobat Mobile (Level: 2, Score: 0.419)\n",
      "2025-07-27 02:55:49,392 - INFO - 56. Learn Acrobat - Generative AI_1.pdf - You Must Sign In To The Zoom Web Portal To Access The Zoom Meeting Information. Then, (Level: 2, Score: 0.417)\n",
      "2025-07-27 02:55:49,392 - INFO - 57. Learn Acrobat - Edit_2.pdf - You Can Split One Or More Pdfs Into Multiple Smaller Pdfs. When Splitting A Pdf, You Can (Level: 2, Score: 0.415)\n",
      "2025-07-27 02:55:49,392 - INFO - 58. Learn Acrobat - Generative AI_1.pdf - The Generative Ai Features Are Only Available In English For Users With Individual Or Teams (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,392 - INFO - 59. Learn Acrobat - Request e-signatures_1.pdf - The Button To Place The Detected Form Fields In The Pdf Document. Alternatively, (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,392 - INFO - 60. Learn Acrobat - Create and Convert_2.pdf - Layers In The Resulting Pdf, Or You Can Flatten The Layers. You Can Also Organize The Visio Layers (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,392 - INFO - 61. Learn Acrobat - Request e-signatures_1.pdf - Data Action. (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,392 - INFO - 62. Learn Acrobat - Request e-signatures_1.pdf - Roaming Ids Can Be Stored On A Server. Acrobat Includes A Default Signature Handler That Can (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,392 - INFO - 63. Learn Acrobat - Create and Convert_1.pdf - Pdfmaker Conversion Settings Vary According To File Types. For Example, The Options Available (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,392 - INFO - 64. Learn Acrobat - Edit_1.pdf - Then Drag The Image To The Desired Location. (Level: 2, Score: 0.414)\n",
      "2025-07-27 02:55:49,396 - INFO - 65. Learn Acrobat - Fill and Sign.pdf - Select Add Initials. Your Initials Appear In The Field. (Level: 2, Score: 0.412)\n",
      "2025-07-27 02:55:49,396 - INFO - 66. Learn Acrobat - Create and Convert_1.pdf - When You Convert Your Visio File, Only Shapes And Guides That Are Printable And Visible In The (Level: 2, Score: 0.411)\n",
      "2025-07-27 02:55:49,396 - INFO - 67. Learn Acrobat - Fill and Sign.pdf - Signature Styles; Click Change Style To View A Different Style. (Level: 2, Score: 0.410)\n",
      "2025-07-27 02:55:49,396 - INFO - 68. Learn Acrobat - Request e-signatures_2.pdf - The Command Is Unavailable If The Signature Is Invalid, Or Is Signed With A Self-Signed (Level: 2, Score: 0.410)\n",
      "2025-07-27 02:55:49,396 - INFO - 69. Learn Acrobat - Create and Convert_2.pdf - In Many Authoring Applications, You Can Use The Print Command With The Adobe Pdf Printer To (Level: 2, Score: 0.409)\n",
      "2025-07-27 02:55:49,398 - INFO - 70. Learn Acrobat - Share_1.pdf - Want To Share The File With. (Level: 2, Score: 0.409)\n",
      "2025-07-27 02:55:49,398 - INFO - 71. Learn Acrobat - Generative AI_1.pdf - Open A File In The Application? (Level: 2, Score: 0.407)\n",
      "2025-07-27 02:55:49,398 - INFO - 72. Learn Acrobat - Export_1.pdf - The Following: (Level: 2, Score: 0.407)\n",
      "2025-07-27 02:55:49,398 - INFO - 73. Learn Acrobat - Edit_1.pdf - The Color You Want Your Text To Change Into. (Level: 2, Score: 0.405)\n",
      "2025-07-27 02:55:49,398 - INFO - 74. Learn Acrobat - Share_1.pdf - Use @Mention In Your Personal Commenting Notes To Start A Review. The @Mentions Includes (Level: 2, Score: 0.403)\n",
      "2025-07-27 02:55:49,398 - INFO - 75. Learn Acrobat - Fill and Sign.pdf - Or Select An Image Of Your Signature. (Level: 2, Score: 0.401)\n",
      "2025-07-27 02:55:49,398 - INFO - 76. Learn Acrobat - Create and Convert_1.pdf - The Acrobat Pdfmaker Conversion Settings Dialog Box Contains The Option That Determines (Level: 2, Score: 0.401)\n",
      "2025-07-27 02:55:49,402 - INFO - 77. Learn Acrobat - Edit_2.pdf - Use The Correct Font And Formatting. When Editing The Document, Ensure That The Font And (Level: 2, Score: 0.401)\n",
      "2025-07-27 02:55:49,402 - INFO - 78. Learn Acrobat - Edit_2.pdf - Document Area. (Level: 2, Score: 0.400)\n",
      "2025-07-27 02:55:49,402 - INFO - 79. Learn Acrobat - Share_2.pdf - Pop-Up Opacity (Level: 2, Score: 0.400)\n",
      "2025-07-27 02:55:49,402 - INFO - 80. Learn Acrobat - Share_2.pdf - By Dragging A Handle. The Knee Line Can Be Resized In One Direction Only; Horizontal Knee Lines (Level: 2, Score: 0.397)\n",
      "2025-07-27 02:55:49,402 - INFO - 81. Learn Acrobat - Share_2.pdf - Expands Vertically As You Type So That All Text Remains Visible. (Level: 2, Score: 0.397)\n",
      "2025-07-27 02:55:49,402 - INFO - 82. Learn Acrobat - Export_1.pdf - Substituted With A Close Match Or Default Font. (Level: 2, Score: 0.396)\n",
      "2025-07-27 02:55:49,402 - INFO - 83. Learn Acrobat - Export_1.pdf - May Look Different When Opened In Acrobat. This Can Happen If The Images Have A Color (Level: 2, Score: 0.395)\n",
      "2025-07-27 02:55:49,406 - INFO - 84. Learn Acrobat - Generative AI_1.pdf - The Chat History Might Still Contain Content From Documents You'Ve Removed From A (Level: 2, Score: 0.394)\n",
      "2025-07-27 02:55:49,406 - INFO - 85. Learn Acrobat - Share_2.pdf - Of The Markup To Indicate The Presence Of Text In The Pop-Up Note. (Level: 2, Score: 0.393)\n",
      "2025-07-27 02:55:49,408 - INFO - 86. Learn Acrobat - Generative AI_1.pdf - With Acrobat. (Level: 2, Score: 0.392)\n",
      "2025-07-27 02:55:49,408 - INFO - 87. Learn Acrobat - Create and Convert_1.pdf - Method 2 (Windows Only): Open The File In Its Source (Level: 2, Score: 0.391)\n",
      "2025-07-27 02:55:49,408 - INFO - 88. Learn Acrobat - Create and Convert_2.pdf - For Headings And Styles Indicate The Element Types. (Level: 2, Score: 0.388)\n",
      "2025-07-27 02:55:49,408 - INFO - 89. Learn Acrobat - Create and Convert_1.pdf - Mail Merges From Word Generate Documents Like Form Letters—For One Common Example— (Level: 2, Score: 0.387)\n",
      "2025-07-27 02:55:49,408 - INFO - 90. Learn Acrobat - Create and Convert_2.pdf - Show This Number Of Recent Archives (Outlook Only) (Level: 2, Score: 0.385)\n",
      "2025-07-27 02:55:49,408 - INFO - 91. Learn Acrobat - Create and Convert_2.pdf - Open The List Of Printers, And Then Click Properties Or Preferences.) (Level: 2, Score: 0.384)\n",
      "2025-07-27 02:55:49,408 - INFO - 92. Learn Acrobat - Share_2.pdf - Font, Font Size (Level: 2, Score: 0.373)\n",
      "2025-07-27 02:55:49,413 - INFO - 93. Learn Acrobat - Edit_2.pdf - File Size (Level: 2, Score: 0.370)\n",
      "2025-07-27 02:55:49,413 - INFO - 94. Learn Acrobat - Create and Convert_1.pdf - Microsoft Office (Level: 2, Score: 0.363)\n",
      "2025-07-27 02:55:49,413 - INFO - 95. Learn Acrobat - Export_1.pdf - Encoding (Level: 2, Score: 0.266)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Adobe Hackathon Round 1B - PDF Document Intelligence System\n",
    "Context-aware extraction and ranking of document sections based on persona and job-to-be-done\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BertHeadingClassifier(nn.Module):\n",
    "    \"\"\"BERT-based classifier for heading detection\"\"\"\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertHeadingClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.projector = nn.Linear(128, 256)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        projected = self.projector(cls_embedding)\n",
    "        logits = self.classifier(projected)\n",
    "        return logits\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document analyzer with BERT-Tiny and LayoutLMv3 models\"\"\"\n",
    "        try:\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model = BertModel.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model.eval()\n",
    "            logger.info(\"BERT-Tiny model loaded successfully\")\n",
    "\n",
    "            self.heading_model = BertHeadingClassifier(self.bert_model)\n",
    "            state_dict = torch.load('./models/document_model.pth', map_location=torch.device('cpu'))\n",
    "            classifier_state = {k.replace('classifier.', ''): v for k, v in state_dict.items() if k.startswith('classifier')}\n",
    "            self.heading_model.classifier.load_state_dict(classifier_state, strict=True)\n",
    "            self.heading_model.eval()\n",
    "            with open('./models/label_encoder.pkl', 'rb') as f:\n",
    "                self.label_encoder = pickle.load(f)\n",
    "            logger.info(\"Round 1A heading detection model loaded successfully\")\n",
    "\n",
    "            self.layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"./models/layoutlmv3\", apply_ocr=False)\n",
    "            self.layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"./models/layoutlmv3\")\n",
    "            self.layoutlm_model.eval()\n",
    "            logger.info(\"LayoutLMv3 model and processor loaded successfully\")\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK punkt data\")\n",
    "                nltk.download('punkt', quiet=True)\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK averaged_perceptron_tagger_eng data\")\n",
    "                nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_domain_terms(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract up to 8 domain-specific terms\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        inputs = self.bert_tokenizer(combined_text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state[0]\n",
    "            tokens = self.bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "        valid_tokens = [token for i, token in enumerate(tokens) if not token.startswith('##') and token not in stop_words and len(token) >= 3 and token not in ['[CLS]', '[SEP]']]\n",
    "        valid_embeddings = [token_embeddings[i].numpy() for i, token in enumerate(tokens) if not token.startswith('##') and token not in stop_words and len(token) >= 3 and token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "        if not valid_tokens:\n",
    "            return []\n",
    "\n",
    "        embeddings_array = np.array(valid_embeddings)\n",
    "        n_clusters = min(8, len(valid_tokens))\n",
    "        if n_clusters < 2:\n",
    "            return valid_tokens[:8]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "        domain_terms = []\n",
    "        tfidf = TfidfVectorizer().fit([combined_text])\n",
    "        vocab = tfidf.vocabulary_\n",
    "        tfidf_scores = tfidf.transform([combined_text]).toarray()[0]\n",
    "\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "            if cluster_indices:\n",
    "                cluster_tokens = [valid_tokens[i] for i in cluster_indices]\n",
    "                token_scores = [(token, tfidf_scores[vocab.get(token, 0)]) for token in cluster_tokens]\n",
    "                token_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                domain_terms.append(token_scores[0][0])\n",
    "\n",
    "        tagged_terms = nltk.pos_tag(domain_terms)\n",
    "        domain_terms = [term for term, pos in tagged_terms if pos.startswith(('NN', 'VB', 'JJ')) and len(term) >= 4]\n",
    "        domain_terms = list(dict.fromkeys(domain_terms))[:8]\n",
    "        if not domain_terms:\n",
    "            domain_terms = valid_tokens[:8]\n",
    "\n",
    "        logger.info(f\"Extracted domain terms: {domain_terms}\")\n",
    "        return domain_terms\n",
    "\n",
    "    def extract_context_keywords(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract context keywords using domain terms\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', combined_text)\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "        keywords = [word for word in words if word not in stop_words and (any(term in word for term in domain_terms) or len(word) >= 5)]\n",
    "        keywords = list(dict.fromkeys(keywords))[:10]\n",
    "        if not keywords:\n",
    "            keywords = [word for word in words if word not in stop_words and len(word) >= 4][:10]\n",
    "        logger.info(f\"Extracted context keywords: {keywords}\")\n",
    "        return keywords\n",
    "\n",
    "    def is_bullet_point(self, text: str) -> bool:\n",
    "        \"\"\"Detect bullet points and list-like structures\"\"\"\n",
    "        bullet_patterns = [\n",
    "            r'^\\s*[•▪▫‣⁃\\u2022]\\s+', r'^\\s*[-*+]\\s+', r'^\\s*\\d+[\\.\\)]\\s+', r'^\\s*[a-zA-Z][\\.\\)]\\s+',\n",
    "            r'^\\s*(tip|note|warning|caution|important):?\\s+'\n",
    "        ]\n",
    "        return any(re.match(pattern, text, re.IGNORECASE) for pattern in bullet_patterns)\n",
    "\n",
    "    def is_valid_heading(self, text: str) -> bool:\n",
    "        \"\"\"Validate heading to reject fragments and bullet points\"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) < 3 or len(text) > 80 or len(text.split()) < 2 or len(text.split()) > 15:\n",
    "            return False\n",
    "        if re.match(r'.*[:,]$', text) or text.lower().startswith(('see ', 'refer ', 'check ', 'visit ')) or self.is_bullet_point(text):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_heading_rule_based(self, line: str, context_keywords: List[str], typography: Dict) -> float:\n",
    "        \"\"\"Rule-based heading detection with relaxed constraints\"\"\"\n",
    "        line = line.strip()\n",
    "        if not line or len(line) < 3 or len(line) > 80 or self.is_bullet_point(line) or not self.is_valid_heading(line):\n",
    "            logger.debug(f\"Line '{line}' rejected: invalid length or format\")\n",
    "            return 0.0\n",
    "\n",
    "        words = line.split()\n",
    "        line_lower = line.lower()\n",
    "        score = 0.0\n",
    "\n",
    "        font_size = typography.get('font_size', 12)\n",
    "        avg_font_size = typography.get('avg_font_size', 12)\n",
    "        is_bold = typography.get('is_bold', False)\n",
    "        if font_size >= avg_font_size * 1.05:\n",
    "            score += 0.35\n",
    "        if is_bold:\n",
    "            score += 0.25\n",
    "        if line.isupper():\n",
    "            score += 0.15\n",
    "        if re.match(r'^[A-Z][a-z]+(\\s[A-Z][a-z]+)*:?$', line):\n",
    "            score += 0.25\n",
    "\n",
    "        if 2 <= len(words) <= 10:\n",
    "            score += 0.25\n",
    "        elif len(words) <= 15:\n",
    "            score += 0.15\n",
    "        if typography.get('has_whitespace_above', False):\n",
    "            score += 0.15\n",
    "        if typography.get('starts_at_margin', True):\n",
    "            score += 0.1\n",
    "\n",
    "        keyword_matches = sum(1 for keyword in context_keywords if keyword in line_lower)\n",
    "        score += min(0.6, keyword_matches * 0.2)\n",
    "        patterns = [\n",
    "            (r'^(chapter|section|part)\\s+\\d+', 0.3),\n",
    "            (r'^(itinerary|activity|destination|sightseeing|guide|tips|experiences|cuisine|culture|adventure|attractions|nightlife)\\s+', 0.4),\n",
    "            (r'^(plan|explore|visit|discover|eat|stay|travel)', 0.25),\n",
    "            (r'(trip|travel|culture|group|cuisine|adventure|friends)', 0.2)\n",
    "        ]\n",
    "        for pattern, weight in patterns:\n",
    "            if re.search(pattern, line_lower):\n",
    "                score += weight\n",
    "\n",
    "        if line.endswith('.'):\n",
    "            score -= 0.15\n",
    "        exclusion_patterns = [\n",
    "            r'^(note|tip|warning|caution|important):?\\s*$',\n",
    "            r'^\\d+\\s*(st|nd|rd|th)\\s+',\n",
    "            r'^(figure|table|diagram|image)\\s+\\d+'\n",
    "        ]\n",
    "        for pattern in exclusion_patterns:\n",
    "            if re.match(pattern, line_lower):\n",
    "                score *= 0.2\n",
    "\n",
    "        logger.debug(f\"Rule-based score for '{line}': {score:.2f}\")\n",
    "        return min(1.0, max(0.0, score))\n",
    "\n",
    "    def is_heading_model_based(self, line: str) -> float:\n",
    "        \"\"\"Model-based heading detection\"\"\"\n",
    "        try:\n",
    "            inputs = self.bert_tokenizer(line, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                logits = self.heading_model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "                probability = torch.softmax(logits, dim=1)[0, 1].item()\n",
    "            logger.debug(f\"BERT score for '{line}': {probability:.2f}\")\n",
    "            return probability\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Model-based heading detection failed: {str(e)}\")\n",
    "            return 0.4\n",
    "\n",
    "    def detect_heading(self, line: str, context_keywords: List[str], typography: Dict, layoutlm_label: str = \"body\") -> bool:\n",
    "        \"\"\"Hybrid heading detection with increased LayoutLMv3 weight\"\"\"\n",
    "        rule_score = self.is_heading_rule_based(line, context_keywords, typography)\n",
    "        model_score = self.is_heading_model_based(line)\n",
    "        is_heading_layoutlm = layoutlm_label == \"heading\"\n",
    "        combined_score = 0.3 * model_score + 0.3 * rule_score + 0.4 * (1.0 if is_heading_layoutlm else 0.0)\n",
    "        threshold = 0.4 if any(keyword in line.lower() for keyword in context_keywords) else 0.5\n",
    "        logger.debug(f\"Heading detection for '{line}': rule_score={rule_score:.2f}, model_score={model_score:.2f}, layoutlm_label={layoutlm_label}, combined_score={combined_score:.2f}, threshold={threshold:.2f}\")\n",
    "        return combined_score > threshold\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str, context_keywords: List[str]) -> List[Dict]:\n",
    "        \"\"\"Extract text and sections with improved heading detection\"\"\"\n",
    "        sections = []\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                avg_font_size = sum(char['size'] for page in pdf.pages for char in page.chars) / max(1, sum(len(page.chars) for page in pdf.pages))\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    text_lines = page.extract_text_lines(return_chars=True) or []\n",
    "                    if not text_lines:\n",
    "                        logger.warning(f\"No text extracted from page {page_num}\")\n",
    "                        continue\n",
    "\n",
    "                    page_image = page.to_image(resolution=300).original.convert(\"RGB\")\n",
    "                    words = []\n",
    "                    boxes = []\n",
    "                    for line in text_lines:\n",
    "                        line_text = line['text'].strip()\n",
    "                        if not line_text or len(line_text) < 3 or re.match(r'^\\d+$', line_text):\n",
    "                            continue\n",
    "                        words.append(line_text)\n",
    "                        x0, y0, x1, y1 = line['x0'], line['top'], line['x1'], line['bottom']\n",
    "                        boxes.append([int(1000 * x0 / page.width), int(1000 * y0 / page.height), int(1000 * x1 / page.width), int(1000 * y1 / page.height)])\n",
    "\n",
    "                    if not words:\n",
    "                        continue\n",
    "\n",
    "                    encoding = self.layoutlm_processor(page_image, words, boxes=boxes, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.layoutlm_model(**encoding)\n",
    "                        logits = outputs.logits\n",
    "                        predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "                    id2label = {0: \"body\", 1: \"heading\"}\n",
    "                    layoutlm_labels = [id2label.get(pred, \"body\") for pred in predictions[:len(words)]]\n",
    "                    logger.debug(f\"LayoutLMv3 labels for page {page_num}: {layoutlm_labels}\")\n",
    "\n",
    "                    current_section = None\n",
    "                    section_text = []\n",
    "                    for line_idx, (line, layoutlm_label) in enumerate(zip(text_lines, layoutlm_labels)):\n",
    "                        line_text = line['text'].strip()\n",
    "                        if not line_text or len(line_text) < 3 or re.match(r'^\\d+$', line_text) or self.is_bullet_point(line_text):\n",
    "                            continue\n",
    "\n",
    "                        chars = line.get('chars', [])\n",
    "                        font_size = max((char['size'] for char in chars), default=12) if chars else 12\n",
    "                        is_bold = any(char.get('fontname', '').lower().find('bold') != -1 for char in chars)\n",
    "                        has_whitespace_above = line.get('top', 0) > page.height * 0.05\n",
    "                        starts_at_margin = line.get('x0', 0) < page.width * 0.05\n",
    "                        typography = {\n",
    "                            'font_size': font_size, 'avg_font_size': avg_font_size, 'is_bold': is_bold,\n",
    "                            'has_whitespace_above': has_whitespace_above, 'starts_at_margin': starts_at_margin\n",
    "                        }\n",
    "\n",
    "                        line_text_normalized = line_text.title()\n",
    "                        if self.detect_heading(line_text, context_keywords, typography, layoutlm_label):\n",
    "                            if current_section and section_text:\n",
    "                                combined_text = ' '.join(section_text).strip()\n",
    "                                if len(combined_text.split()) >= 10:\n",
    "                                    sections.append({\n",
    "                                        'document': os.path.basename(pdf_path),\n",
    "                                        'page': current_section['page'],\n",
    "                                        'section_title': current_section['title'],\n",
    "                                        'text': combined_text,\n",
    "                                        'level': current_section['level']\n",
    "                                    })\n",
    "                            level = 1 if font_size >= avg_font_size * 1.1 else 2\n",
    "                            current_section = {'title': line_text_normalized, 'page': page_num, 'level': level}\n",
    "                            section_text = []\n",
    "                        else:\n",
    "                            if current_section:\n",
    "                                section_text.append(line_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "            try:\n",
    "                with pdfplumber.open(pdf_path) as pdf:\n",
    "                    fallback_count = 0\n",
    "                    for page_num, page in enumerate(pdf.pages, 1):\n",
    "                        text = page.extract_text()\n",
    "                        if text and len(text.strip()) > 50 and fallback_count < 5:\n",
    "                            paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 20]\n",
    "                            for para in paragraphs[:3]:\n",
    "                                first_line = para.split('\\n')[0].strip()\n",
    "                                if len(first_line.split()) >= 2 and len(para.split()) >= 10 and self.is_valid_heading(first_line):\n",
    "                                    chars = page.chars or []\n",
    "                                    font_size = max((char['size'] for char in chars), default=12) if chars else 12\n",
    "                                    if font_size >= 12:\n",
    "                                        sections.append({\n",
    "                                            'document': os.path.basename(pdf_path),\n",
    "                                            'page': page_num,\n",
    "                                            'section_title': first_line.title(),\n",
    "                                            'text': para,\n",
    "                                            'level': 3\n",
    "                                        })\n",
    "                                        fallback_count += 1\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(f\"Fallback extraction failed: {str(fallback_error)}\")\n",
    "\n",
    "        merged_sections = []\n",
    "        section_map = {}\n",
    "        for section in sections:\n",
    "            key = (section['document'], section['section_title'].lower(), section['page'])\n",
    "            if key in section_map:\n",
    "                section_map[key]['text'] += ' ' + section['text']\n",
    "            else:\n",
    "                section_map[key] = section\n",
    "        merged_sections = list(section_map.values())\n",
    "\n",
    "        logger.info(f\"Extracted {len(merged_sections)} sections from {pdf_path}\")\n",
    "        return merged_sections[:15]\n",
    "\n",
    "    def encode_text_bert(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        \"\"\"Encode texts with mean pooling\"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = [re.sub(r'\\s+', ' ', t.strip())[:500] for t in texts[i:i + batch_size]]\n",
    "            inputs = self.bert_tokenizer(batch_texts, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                embeddings.extend(batch_embeddings.numpy())\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_relevance_scores(self, sections: List[Dict], persona: str, job_to_be_done: str, context_keywords: List[str]) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"Compute relevance scores with enhanced keyword weighting\"\"\"\n",
    "        if not sections:\n",
    "            return []\n",
    "\n",
    "        logger.info(f\"Computing relevance scores for {len(sections)} sections\")\n",
    "        keywords_str = \" \".join(context_keywords)\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Focus on: {keywords_str}\"\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "        section_texts = [f\"{section['section_title']} {section['text']}\" for section in sections]\n",
    "        all_texts = [query] + section_texts\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        section_embeddings = embeddings[1:]\n",
    "\n",
    "        similarities = cosine_similarity(query_embedding, section_embeddings)[0]\n",
    "        adjusted_scores = []\n",
    "        for i, (sim, section) in enumerate(zip(similarities, sections)):\n",
    "            word_count = len(section['text'].split())\n",
    "            length_factor = 0.5 if word_count < 20 else min(1.5, 1 + 0.3 * np.log(word_count / 20))\n",
    "            title_quality = 1.5 if any(word in section['section_title'].lower() for word in domain_terms) else 0.7 if len(section['section_title'].split()) < 2 else 1.0\n",
    "            section_content = f\"{section['section_title']} {section['text']}\".lower()\n",
    "            keyword_matches = sum(1 for keyword in context_keywords + domain_terms if keyword in section_content)\n",
    "            keyword_boost = 3.0 if keyword_matches >= 3 else 2.0 if keyword_matches == 2 else 1.5 if keyword_matches == 1 else 1.0\n",
    "            level_boost = 1.2 if section['level'] == 1 else 1.0 if section['level'] == 2 else 0.8\n",
    "            domain_boost = 1 + sum(0.15 for term in domain_terms if term in section_content)\n",
    "            adjusted_score = sim * length_factor * title_quality * keyword_boost * level_boost * domain_boost\n",
    "            adjusted_scores.append(adjusted_score)\n",
    "\n",
    "        scored_sections = list(zip(sections, adjusted_scores))\n",
    "        scored_sections.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored_sections[:max(7, len(scored_sections))]\n",
    "\n",
    "    def extract_key_sentences(self, text: str, persona: str, job_to_be_done: str, context_keywords: List[str], top_k: int = 5) -> str:\n",
    "        global domain_terms\n",
    "        \n",
    "        \"\"\"Extract key sentences ensuring multi-sentence output\"\"\"\n",
    "        sentences = [s.strip() for s in nltk.sent_tokenize(text) if len(s.split()) >= 15 and len(s) >= 50 and not s.lower().startswith(('note:', 'tip:', 'warning:', 'caution:')) and not s.endswith(('...', ','))]\n",
    "        if len(sentences) < 2:\n",
    "            paragraphs = re.split(r'\\n{2,}', text)\n",
    "            combined_text = ' '.join([p.strip() for p in paragraphs if len(p.strip().split()) >= 15][:3])\n",
    "            sentences = [s.strip() for s in nltk.sent_tokenize(combined_text) if len(s.split()) >= 15 and len(s) >= 50 and not s.endswith(('...', ','))]\n",
    "            if len(sentences) < 2:\n",
    "                return combined_text[:400] if len(combined_text) > 50 else combined_text\n",
    "\n",
    "        logger.info(f\"Extracting {top_k} key sentences from {len(sentences)} sentences\")\n",
    "        keywords_str = \" \".join(context_keywords)\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Key aspects: {keywords_str}\"\n",
    "        all_texts = [query] + sentences\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        sentence_embeddings = embeddings[1:]\n",
    "        similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        adjusted_similarities = []\n",
    "        for i, (sim, sentence) in enumerate(zip(similarities, sentences)):\n",
    "            keyword_boost = 1 + sum(0.25 for keyword in context_keywords if keyword in sentence.lower())\n",
    "            # action_boost = 0.2 if any(word in sentence.lower() for word in domain_terms) else 0.0\n",
    "            adjusted_similarities.append(sim * keyword_boost * (1))\n",
    "\n",
    "        sentence_scores = list(enumerate(adjusted_similarities))\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_indices = [idx for idx, _ in sentence_scores[:max(top_k, 2)]]\n",
    "        top_indices.sort()\n",
    "        return ' '.join(sentences[i] for i in top_indices)\n",
    "\n",
    "def load_input_config(input_dir: str) -> Tuple[str, str]:\n",
    "    \"\"\"Load and validate input configuration\"\"\"\n",
    "    config_path = os.path.join(input_dir, \"input.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"input.json not found at {config_path}\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    persona = config.get('persona')\n",
    "    job_to_be_done = config.get('job_to_be_done')\n",
    "    if isinstance(persona, dict):\n",
    "        persona = persona.get('role') or persona.get('name')\n",
    "    if isinstance(job_to_be_done, dict):\n",
    "        job_to_be_done = job_to_be_done.get('task') or job_to_be_done.get('description')\n",
    "    if not persona or not job_to_be_done:\n",
    "        raise ValueError(\"Both 'persona' and 'job_to_be_done' must be specified in input.json\")\n",
    "    return str(persona), str(job_to_be_done)\n",
    "\n",
    "def find_pdf_files(pdfs_dir: str) -> List[str]:\n",
    "    \"\"\"Find and validate PDF files\"\"\"\n",
    "    if not os.path.exists(pdfs_dir):\n",
    "        raise FileNotFoundError(f\"PDFs directory not found: {pdfs_dir}\")\n",
    "    pdf_files = [f for f in os.listdir(pdfs_dir) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in {pdfs_dir}\")\n",
    "    return [os.path.join(pdfs_dir, f) for f in pdf_files]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process documents and generate output\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        input_dir = \"./input\"\n",
    "        pdfs_dir = os.path.join(input_dir, \"PDFs\")\n",
    "        output_dir = \"./output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Loading input configuration\")\n",
    "        persona, job_to_be_done = load_input_config(input_dir)\n",
    "        logger.info(f\"Persona: {persona}\")\n",
    "        logger.info(f\"Job to be done: {job_to_be_done}\")\n",
    "\n",
    "        pdf_paths = find_pdf_files(pdfs_dir)\n",
    "        pdf_files = [os.path.basename(path) for path in pdf_paths]\n",
    "        logger.info(f\"Processing {len(pdf_files)} documents: {pdf_files}\")\n",
    "\n",
    "        logger.info(\"Initializing document analyzer\")\n",
    "        analyzer = DocumentAnalyzer()\n",
    "\n",
    "        context_keywords = analyzer.extract_context_keywords(persona, job_to_be_done)\n",
    "        all_sections = []\n",
    "        for pdf_path in pdf_paths:\n",
    "            sections = analyzer.extract_text_from_pdf(pdf_path, context_keywords)\n",
    "            all_sections.extend(sections)\n",
    "\n",
    "        if not all_sections:\n",
    "            raise ValueError(\"No sections extracted from any document\")\n",
    "\n",
    "        logger.info(f\"Total sections extracted: {len(all_sections)}\")\n",
    "        scored_sections = analyzer.compute_relevance_scores(all_sections, persona, job_to_be_done, context_keywords)\n",
    "        top_sections = scored_sections\n",
    "\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"input_documents\": pdf_files,\n",
    "                \"persona\": persona,\n",
    "                \"job_to_be_done\": job_to_be_done,\n",
    "                \"context_keywords\": context_keywords,\n",
    "                \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_sections_processed\": len(all_sections)\n",
    "            },\n",
    "            \"extracted_sections\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"section_title\": section['section_title'],\n",
    "                    \"importance_rank\": rank,\n",
    "                    \"page_number\": section['page'],\n",
    "                    \"heading_level\": section['level']\n",
    "                } for rank, (section, _) in enumerate(top_sections, 1)\n",
    "            ],\n",
    "            \"sub_section_analysis\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"section_title\": section['section_title'],\n",
    "                    \"refined_text\": analyzer.extract_key_sentences(\n",
    "                        section['text'], persona, job_to_be_done, context_keywords\n",
    "                    ),\n",
    "                    \"page_number\": section['page']\n",
    "                } for section, _ in top_sections\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"output.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "        logger.info(f\"Output saved to {output_path}\")\n",
    "        for rank, (section, score) in enumerate(top_sections, 1):\n",
    "            logger.info(f\"{rank}. {section['document']} - {section['section_title']} (Level: {section['level']}, Score: {score:.3f})\")\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#GG CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3680c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
