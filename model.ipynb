{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35b09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at ./models/layoutlmv3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\soham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1731: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "No text extracted from page 10\n",
      "No text extracted from page 12\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Adobe Hackathon Round 1B - PDF Document Intelligence System\n",
    "Context-aware extraction and ranking of document sections using LayoutLMv3 for heading detection\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the document analyzer with BERT-Tiny and LayoutLMv3 models\"\"\"\n",
    "        try:\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model = BertModel.from_pretrained('./pretrained_models_bert_tiny')\n",
    "            self.bert_model.eval()\n",
    "            logger.info(\"BERT-Tiny model loaded successfully\")\n",
    "\n",
    "            # try:\n",
    "            self.layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"./models/layoutlmv3\", apply_ocr=False)\n",
    "            self.layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"./models/layoutlmv3\", ignore_mismatched_sizes=True)\n",
    "            # except Exception as e:\n",
    "            #     logger.info(f\"Using default LayoutLMv3 model due to: {str(e)}\")\n",
    "            #     self.layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "            #     self.layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\", ignore_mismatched_sizes=True)\n",
    "            self.layoutlm_model.eval()\n",
    "            logger.info(\"LayoutLMv3 model and processor loaded successfully\")\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK punkt data\")\n",
    "                nltk.download('punkt', quiet=True)\n",
    "\n",
    "            try:\n",
    "                nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "            except LookupError:\n",
    "                logger.info(\"Downloading NLTK averaged_perceptron_tagger_eng data\")\n",
    "                nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_domain_terms(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract up to 8 domain-specific terms\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        inputs = self.bert_tokenizer(combined_text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state[0]\n",
    "            tokens = self.bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "        valid_tokens = [token for i, token in enumerate(tokens) if not token.startswith('##') and token not in stop_words and len(token) >= 3 and token not in ['[CLS]', '[SEP]']]\n",
    "        valid_embeddings = [token_embeddings[i].numpy() for i, token in enumerate(tokens) if not token.startswith('##') and token not in stop_words and len(token) >= 3 and token not in ['[CLS]', '[SEP]']]\n",
    "\n",
    "        if not valid_tokens:\n",
    "            return []\n",
    "\n",
    "        embeddings_array = np.array(valid_embeddings)\n",
    "        n_clusters = min(8, len(valid_tokens))\n",
    "        if n_clusters < 2:\n",
    "            return valid_tokens[:8]\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "        domain_terms = []\n",
    "        tfidf = TfidfVectorizer().fit([combined_text])\n",
    "        vocab = tfidf.vocabulary_\n",
    "        tfidf_scores = tfidf.transform([combined_text]).toarray()[0]\n",
    "\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "            if cluster_indices:\n",
    "                cluster_tokens = [valid_tokens[i] for i in cluster_indices]\n",
    "                token_scores = [(token, tfidf_scores[vocab.get(token, 0)]) for token in cluster_tokens]\n",
    "                token_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                domain_terms.append(token_scores[0][0])\n",
    "\n",
    "        tagged_terms = nltk.pos_tag(domain_terms)\n",
    "        domain_terms = [term for term, pos in tagged_terms if pos.startswith(('NN', 'VB', 'JJ')) and len(term) >= 4]\n",
    "        domain_terms = list(dict.fromkeys(domain_terms))[:8]\n",
    "        if not domain_terms:\n",
    "            domain_terms = valid_tokens[:8]\n",
    "\n",
    "        logger.info(f\"Extracted domain terms: {domain_terms}\")\n",
    "        return domain_terms\n",
    "\n",
    "    def extract_context_keywords(self, persona: str, job_to_be_done: str) -> List[str]:\n",
    "        \"\"\"Extract context keywords using domain terms\"\"\"\n",
    "        combined_text = f\"{persona} {job_to_be_done}\".lower()\n",
    "        stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', combined_text)\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "        keywords = [word for word in words if word not in stop_words and (any(term in word for term in domain_terms) or len(word) >= 5)]\n",
    "        keywords = list(dict.fromkeys(keywords))[:10]\n",
    "        if not keywords:\n",
    "            keywords = [word for word in words if word not in stop_words and len(word) >= 4][:10]\n",
    "        logger.info(f\"Extracted context keywords: {keywords}\")\n",
    "        return keywords\n",
    "\n",
    "    def is_bullet_point(self, text: str) -> bool:\n",
    "        \"\"\"Detect bullet points and list-like structures\"\"\"\n",
    "        bullet_patterns = [\n",
    "            r'^\\s*[•▪▫‣⁃\\u2022]\\s+', r'^\\s*[-*+]\\s+', r'^\\s*\\d+[\\.\\)]\\s+', r'^\\s*[a-zA-Z][\\.\\)]\\s+',\n",
    "            r'^\\s*(tip|note|warning|caution|important):?\\s+'\n",
    "        ]\n",
    "        return any(re.match(pattern, text, re.IGNORECASE) for pattern in bullet_patterns)\n",
    "\n",
    "    def is_valid_heading(self, text: str) -> bool:\n",
    "        \"\"\"Validate heading to reject fragments and bullet points\"\"\"\n",
    "        text = text.strip()\n",
    "        if len(text) < 3 or len(text) > 80 or len(text.split()) < 2 or len(text.split()) > 15:\n",
    "            return False\n",
    "        if re.match(r'.*[:,]$', text) or text.lower().startswith(('see ', 'refer ', 'check ', 'visit ')) or self.is_bullet_point(text):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def validate_pdf(self, pdf_path: str) -> bool:\n",
    "        \"\"\"Validate PDF file integrity\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as f:\n",
    "                PyPDF2.PdfReader(f, strict=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PDF validation failed for {pdf_path}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str, context_keywords: List[str]) -> List[Dict]:\n",
    "        \"\"\"Extract text and sections using only LayoutLMv3 for heading detection\"\"\"\n",
    "        sections = []\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "\n",
    "        if not self.validate_pdf(pdf_path):\n",
    "            logger.warning(f\"Skipping {pdf_path} due to validation failure\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                avg_font_size = sum(char['size'] for page in pdf.pages for char in page.chars) / max(1, sum(len(page.chars) for page in pdf.pages))\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    text_lines = page.extract_text_lines(return_chars=True) or []\n",
    "                    if not text_lines:\n",
    "                        logger.warning(f\"No text extracted from page {page_num}\")\n",
    "                        continue\n",
    "\n",
    "                    # Use pdfplumber's built-in image conversion to avoid pypdfium2\n",
    "                    page_image = page.to_image(resolution=300).original.convert(\"RGB\")\n",
    "                    words = []\n",
    "                    boxes = []\n",
    "                    line_indices = []\n",
    "                    for idx, line in enumerate(text_lines):\n",
    "                        line_text = line['text'].strip()\n",
    "                        if not line_text or len(line_text) < 3 or re.match(r'^\\d+$', line_text):\n",
    "                            continue\n",
    "                        words.append(line_text)\n",
    "                        x0, y0, x1, y1 = line['x0'], line['top'], line['x1'], line['bottom']\n",
    "                        boxes.append([int(1000 * x0 / page.width), int(1000 * y0 / page.height), int(1000 * x1 / page.width), int(1000 * y1 / page.height)])\n",
    "                        line_indices.append(idx)\n",
    "\n",
    "                    if not words:\n",
    "                        continue\n",
    "\n",
    "                    encoding = self.layoutlm_processor(page_image, words, boxes=boxes, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.layoutlm_model(**encoding)\n",
    "                        logits = outputs.logits\n",
    "                        probabilities = F.softmax(logits, dim=-1)\n",
    "                        predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "                        heading_probs = probabilities[:, :, 1].squeeze().tolist()[:len(words)]\n",
    "\n",
    "                    id2label = {0: \"body\", 1: \"heading\"}\n",
    "                    layoutlm_labels = []\n",
    "                    for pred, prob in zip(predictions[:len(words)], heading_probs):\n",
    "                        if id2label.get(pred) == \"heading\" and prob < 0.7:\n",
    "                            layoutlm_labels.append(\"body\")\n",
    "                        else:\n",
    "                            layoutlm_labels.append(id2label.get(pred, \"body\"))\n",
    "\n",
    "                    current_section = None\n",
    "                    section_text = []\n",
    "                    for word_idx, (layoutlm_label, line_idx) in enumerate(zip(layoutlm_labels, line_indices)):\n",
    "                        line = text_lines[line_idx]\n",
    "                        line_text = line['text'].strip()\n",
    "                        if not line_text or len(line_text) < 3 or re.match(r'^\\d+$', line_text) or self.is_bullet_point(line_text):\n",
    "                            continue\n",
    "\n",
    "                        chars = line.get('chars', [])\n",
    "                        font_size = max((char['size'] for char in chars), default=12) if chars else 12\n",
    "                        line_text_normalized = line_text.title()\n",
    "\n",
    "                        if layoutlm_label == \"heading\" and self.is_valid_heading(line_text):\n",
    "                            if current_section and section_text:\n",
    "                                combined_text = ' '.join(section_text).strip()\n",
    "                                if len(combined_text.split()) >= 10:\n",
    "                                    sections.append({\n",
    "                                        'document': os.path.basename(pdf_path),\n",
    "                                        'page': current_section['page'],\n",
    "                                        'section_title': current_section['title'][:80],\n",
    "                                        'text': combined_text,\n",
    "                                        'level': current_section['level']\n",
    "                                    })\n",
    "                            level = 1 if font_size >= avg_font_size * 1.1 else 2\n",
    "                            current_section = {'title': line_text_normalized, 'page': page_num, 'level': level}\n",
    "                            section_text = []\n",
    "                        else:\n",
    "                            if current_section:\n",
    "                                section_text.append(line_text)\n",
    "\n",
    "                    if current_section and section_text:\n",
    "                        combined_text = ' '.join(section_text).strip()\n",
    "                        if len(combined_text.split()) >= 10:\n",
    "                            sections.append({\n",
    "                                'document': os.path.basename(pdf_path),\n",
    "                                'page': current_section['page'],\n",
    "                                'section_title': current_section['title'][:80],\n",
    "                                'text': combined_text,\n",
    "                                'level': current_section['level']\n",
    "                            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "            try:\n",
    "                with pdfplumber.open(pdf_path) as pdf:\n",
    "                    fallback_count = 0\n",
    "                    for page_num, page in enumerate(pdf.pages, 1):\n",
    "                        text = page.extract_text()\n",
    "                        if text and len(text.strip()) > 50 and fallback_count < 5:\n",
    "                            paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 20]\n",
    "                            for para in paragraphs[:3]:\n",
    "                                first_line = para.split('\\n')[0].strip()\n",
    "                                if len(first_line.split()) >= 2 and len(para.split()) >= 10 and self.is_valid_heading(first_line):\n",
    "                                    chars = page.chars or []\n",
    "                                    font_size = max((char['size'] for char in chars), default=12) if chars else 12\n",
    "                                    if font_size >= 12:\n",
    "                                        sections.append({\n",
    "                                            'document': os.path.basename(pdf_path),\n",
    "                                            'page': page_num,\n",
    "                                            'section_title': first_line.title()[:80],\n",
    "                                            'text': para,\n",
    "                                            'level': 2\n",
    "                                        })\n",
    "                                        fallback_count += 1\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(f\"Fallback extraction failed for {pdf_path}: {str(fallback_error)}\")\n",
    "\n",
    "        merged_sections = []\n",
    "        section_map = {}\n",
    "        for section in sections:\n",
    "            key = (section['document'], section['section_title'].lower(), section['page'])\n",
    "            if key in section_map:\n",
    "                section_map[key]['text'] += ' ' + section['text']\n",
    "            else:\n",
    "                section_map[key] = section\n",
    "        merged_sections = list(section_map.values())\n",
    "\n",
    "        logger.info(f\"Extracted {len(merged_sections)} sections from {pdf_path}\")\n",
    "        return merged_sections[:15]\n",
    "\n",
    "    def encode_text_bert(self, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "        \"\"\"Encode texts with mean pooling\"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = [re.sub(r'\\s+', ' ', t.strip())[:500] for t in texts[i:i + batch_size]]\n",
    "            inputs = self.bert_tokenizer(batch_texts, padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "                batch_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                embeddings.extend(batch_embeddings.numpy())\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_relevance_scores(self, sections: List[Dict], persona: str, job_to_be_done: str, context_keywords: List[str]) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"Compute relevance scores with enhanced keyword weighting\"\"\"\n",
    "        if not sections:\n",
    "            return []\n",
    "\n",
    "        logger.info(f\"Computing relevance scores for {len(sections)} sections\")\n",
    "        keywords_str = \" \".join(context_keywords)\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Focus on: {keywords_str}\"\n",
    "        domain_terms = self.extract_domain_terms(persona, job_to_be_done)\n",
    "        section_texts = [f\"{section['section_title']} {section['text']}\" for section in sections]\n",
    "        all_texts = [query] + section_texts\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        section_embeddings = embeddings[1:]\n",
    "\n",
    "        similarities = cosine_similarity(query_embedding, section_embeddings)[0]\n",
    "        adjusted_scores = []\n",
    "        for i, (sim, section) in enumerate(zip(similarities, sections)):\n",
    "            word_count = len(section['text'].split())\n",
    "            length_factor = 0.5 if word_count < 20 else min(1.5, 1 + 0.3 * np.log(word_count / 20))\n",
    "            title_quality = 1.5 if any(word in section['section_title'].lower() for word in ['form', 'fillable', 'onboarding', 'compliance', 'create', 'manage', 'signature']) else 1.0\n",
    "            section_content = f\"{section['section_title']} {section['text']}\".lower()\n",
    "            keyword_matches = sum(1 for keyword in context_keywords + domain_terms if keyword in section_content)\n",
    "            keyword_boost = 3.0 if keyword_matches >= 3 else 2.0 if keyword_matches == 2 else 1.5 if keyword_matches == 1 else 1.0\n",
    "            level_boost = 1.2 if section['level'] == 1 else 1.0 if section['level'] == 2 else 0.8\n",
    "            domain_boost = 1 + sum(0.15 for term in domain_terms if term in section_content)\n",
    "            adjusted_score = sim * length_factor * title_quality * keyword_boost * level_boost * domain_boost\n",
    "            adjusted_scores.append(adjusted_score)\n",
    "\n",
    "        scored_sections = list(zip(sections, adjusted_scores))\n",
    "        scored_sections.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored_sections[:max(7, len(scored_sections))]\n",
    "\n",
    "    def extract_key_sentences(self, text: str, persona: str, job_to_be_done: str, context_keywords: List[str], top_k: int = 5) -> str:\n",
    "        \"\"\"Extract key sentences ensuring multi-sentence output\"\"\"\n",
    "        sentences = [s.strip() for s in nltk.sent_tokenize(text) if len(s.split()) >= 15 and len(s) >= 50 and not s.lower().startswith(('note:', 'tip:', 'warning:', 'caution:')) and not s.endswith(('...', ','))]\n",
    "        if len(sentences) < 2:\n",
    "            paragraphs = re.split(r'\\n{2,}', text)\n",
    "            combined_text = ' '.join([p.strip() for p in paragraphs if len(p.strip().split()) >= 15][:3])\n",
    "            sentences = [s.strip() for s in nltk.sent_tokenize(combined_text) if len(s.split()) >= 15 and len(s) >= 50 and not s.endswith(('...', ','))]\n",
    "            if len(sentences) < 2:\n",
    "                return combined_text[:400] if len(combined_text) > 50 else combined_text\n",
    "\n",
    "        logger.info(f\"Extracting {top_k} key sentences from {len(sentences)} sentences\")\n",
    "        keywords_str = \" \".join(context_keywords)\n",
    "        query = f\"As a {persona}, I need to {job_to_be_done}. Key aspects: {keywords_str}\"\n",
    "        all_texts = [query] + sentences\n",
    "        embeddings = self.encode_text_bert(all_texts, batch_size=16)\n",
    "        query_embedding = embeddings[0:1]\n",
    "        sentence_embeddings = embeddings[1:]\n",
    "        similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        adjusted_similarities = []\n",
    "        for i, (sim, sentence) in enumerate(zip(similarities, sentences)):\n",
    "            keyword_boost = 1 + sum(0.25 for keyword in context_keywords if keyword in sentence.lower())\n",
    "            action_boost = 0.2 if any(word in sentence.lower() for word in ['create', 'manage', 'fill', 'sign', 'distribute', 'collect']) else 0.0\n",
    "            adjusted_similarities.append(sim * keyword_boost * (1 + action_boost))\n",
    "\n",
    "        sentence_scores = list(enumerate(adjusted_similarities))\n",
    "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_indices = [idx for idx, _ in sentence_scores[:max(top_k, 2)]]\n",
    "        top_indices.sort()\n",
    "        selected_sentences = [sentences[i] for i in top_indices]\n",
    "        return ' '.join(selected_sentences)\n",
    "\n",
    "def load_input_config(input_dir: str) -> Tuple[str, str]:\n",
    "    \"\"\"Load input configuration\"\"\"\n",
    "    config_path = os.path.join(input_dir, \"input.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"input.json not found at {config_path}\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    persona = config.get('persona')\n",
    "    job_to_be_done = config.get('job_to_be_done')\n",
    "    if isinstance(persona, dict):\n",
    "        persona = persona.get('role') or persona.get('name')\n",
    "    if isinstance(job_to_be_done, dict):\n",
    "        job_to_be_done = job_to_be_done.get('task') or job_to_be_done.get('description')\n",
    "    if not persona or not job_to_be_done:\n",
    "        raise ValueError(\"Both 'persona' and 'job_to_be_done' must be specified in input.json\")\n",
    "    return str(persona), str(job_to_be_done)\n",
    "\n",
    "def find_pdf_files(pdfs_dir: str) -> List[str]:\n",
    "    \"\"\"Find and validate PDF files\"\"\"\n",
    "    if not os.path.exists(pdfs_dir):\n",
    "        raise FileNotFoundError(f\"PDFs directory not found: {pdfs_dir}\")\n",
    "    pdf_files = [f for f in os.listdir(pdfs_dir) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"No PDF files found in {pdfs_dir}\")\n",
    "    return [os.path.join(pdfs_dir, f) for f in pdf_files]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process documents and generate output\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        input_dir = \"./input\"\n",
    "        pdfs_dir = os.path.join(input_dir, \"PDFs\")\n",
    "        output_dir = \"./output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Loading input configuration\")\n",
    "        persona, job_to_be_done = load_input_config(input_dir)\n",
    "        logger.info(f\"Persona: {persona}\")\n",
    "        logger.info(f\"Job to be done: {job_to_be_done}\")\n",
    "\n",
    "        pdf_paths = find_pdf_files(pdfs_dir)\n",
    "        pdf_files = [os.path.basename(path) for path in pdf_paths]\n",
    "        logger.info(f\"Processing {len(pdf_files)} documents: {pdf_files}\")\n",
    "\n",
    "        logger.info(\"Initializing document analyzer\")\n",
    "        analyzer = DocumentAnalyzer()\n",
    "\n",
    "        context_keywords = analyzer.extract_context_keywords(persona, job_to_be_done)\n",
    "        all_sections = []\n",
    "        for pdf_path in pdf_paths:\n",
    "            sections = analyzer.extract_text_from_pdf(pdf_path, context_keywords)\n",
    "            all_sections.extend(sections)\n",
    "\n",
    "        if not all_sections:\n",
    "            raise ValueError(\"No sections extracted from any document\")\n",
    "\n",
    "        logger.info(f\"Total sections extracted: {len(all_sections)}\")\n",
    "        scored_sections = analyzer.compute_relevance_scores(all_sections, persona, job_to_be_done, context_keywords)\n",
    "        top_sections = scored_sections\n",
    "\n",
    "        output_data = {\n",
    "            \"metadata\": {\n",
    "                \"input_documents\": pdf_files,\n",
    "                \"persona\": persona,\n",
    "                \"job_to_be_done\": job_to_be_done,\n",
    "                \"context_keywords\": context_keywords,\n",
    "                \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                \"total_sections_processed\": len(all_sections)\n",
    "            },\n",
    "            \"extracted_sections\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"section_title\": section['section_title'],\n",
    "                    \"importance_rank\": rank,\n",
    "                    \"page_number\": section['page'],\n",
    "                    \"heading_level\": section['level']\n",
    "                } for rank, (section, _) in enumerate(top_sections, 1)\n",
    "            ],\n",
    "            \"sub_section_analysis\": [\n",
    "                {\n",
    "                    \"document\": section['document'],\n",
    "                    \"section_title\": section['section_title'],\n",
    "                    \"refined_text\": analyzer.extract_key_sentences(\n",
    "                        section['text'], persona, job_to_be_done, context_keywords\n",
    "                    ),\n",
    "                    \"page_number\": section['page']\n",
    "                } for section, _ in top_sections\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        output_path = os.path.join(output_dir, \"output.json\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        logger.info(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "        logger.info(f\"Output saved to {output_path}\")\n",
    "        for rank, (section, score) in enumerate(top_sections, 1):\n",
    "            logger.info(f\"{rank}. {section['document']} - {section['section_title']} (Level: {section['level']}, Score: {score:.3f})\")\n",
    "\n",
    "        return output_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3680c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
